{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2U0y6_MtZ9I",
    "outputId": "8dce3897-0149-490c-fe59-8be822fcaa8a"
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KLz5M7sFpO0W"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/31 18:46:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "# Import the time module so we can time our queries.\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").config(\"spark.driver.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CahJMb3cpWdW",
    "outputId": "4d0c57d7-87c3-4463-d0f5-3abfbd2a9e16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Read in data from S3 Bucket\n",
    "from pyspark import SparkFiles\n",
    "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/3/NYC_Building_Violations.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"NYC_Building_Violations.csv\"), sep=\",\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S62v26TZLHso",
    "outputId": "5480a615-9014-42fe-ebd9-406add28c876"
   },
   "outputs": [],
   "source": [
    "# Get a summary of the data. \n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7fsnqRDqG2C",
    "outputId": "95a31a73-2ffb-430d-fb22-ec062d106ec6"
   },
   "outputs": [],
   "source": [
    " # Let's create a view with our DataFrame and run SQL that will sum up the boroughs by the type of violation.\n",
    "# We can output the time this step runs in seconds.\n",
    "# Because we are timing the executions, remember to run twice to eliminate the \"load time\" from the discussion.\n",
    "\n",
    "df.createOrReplaceTempView('violations')\n",
    "start_time = time.time()\n",
    "\n",
    "spark.sql(\"\"\"select VIOLATION_TYPE, sum(BORO) from violations group by 1\"\"\").show()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g04Evw2jqHoK"
   },
   "outputs": [],
   "source": [
    "# Write out the data in parquet format\n",
    "# Note: That this is pretty much the same as writing out to a csv to your local directory.\n",
    "# We are telling Spark to overwrite all of the data if it already exists\n",
    "df.write.parquet('parquet_violations', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYiTD19bHuV4"
   },
   "source": [
    "\n",
    "\n",
    "*   click the folder icon on the left of the notebook to expose the folders and files stored in your colab enviornment.  Notice that a new folder is present with the same name as your parquet file (parquet_title_basic)\n",
    "*   inside of it you will find 'part-*.parquet' files and a '_SUCCESS' file. \n",
    "*  The '_SUCCESS' file is created when Spark creates a Parquet folder\n",
    "*  the part-* files are binary files that store your compressed data in columnar format\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SaDjaQXqnPI"
   },
   "outputs": [],
   "source": [
    "# Read in our new parquet formatted data\n",
    "p_df=spark.read.parquet('parquet_violations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sT7d4hu-q32d"
   },
   "outputs": [],
   "source": [
    "# A parquet formatted DataFrame has all the same methods as a row-based DataFrame\n",
    "# We can convert the DataFrame to a view.\n",
    "p_df.createOrReplaceTempView('p_violations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwpUfAoeq71b",
    "outputId": "d6850e76-4e87-4478-bb41-922a51edc6ce"
   },
   "outputs": [],
   "source": [
    "# Run the same sql as above.  (Note: If you have small datasets it IS possible that times may be very close.)\n",
    "# Because we are timing the executions, remember to run twice to eliminate the \"load time\" from the discussion.\n",
    "\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"select VIOLATION_TYPE, sum(BORO) from p_violations group by 1\"\"\").show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZY1lKBHrjLg"
   },
   "outputs": [],
   "source": [
    "# Writing out a csv file from Spark will also create a folder with \"part\" files.\n",
    "# These files are not binary or compressed and in reality are just normal csv files broken into partitions.\n",
    "# You can see the folder 'out_violations.csv' in your local directory.\n",
    "df.write.csv('out_violations.csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a parquet file into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zit0gXHn4Hf2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the parquet_violations folder and get the name of a file and edit the path to the parquet file.  \n",
    "# Check for the correct file name, since the file number will change.\n",
    "parquet_file = \"parquet_violations/part-00000-222c5822-8fe4-4450-97d0-65b5d4a334b4-c000.snappy.parquet\"\n",
    "\n",
    "# Convert the parquet file to a Pandas DataFrame. \n",
    "part_00000_df = pd.read_parquet(parquet_file, engine='auto')\n",
    "part_00000_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Read_and_Write_Parquet_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
